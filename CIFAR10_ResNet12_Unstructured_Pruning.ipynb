{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tqm342Kya-5",
        "outputId": "f1338173-082f-4276-ab57-f6fb7b8df71c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.7/364.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting flwr_datasets[vision]\n",
            "  Downloading flwr_datasets-0.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m668.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets<2.20.0,>=2.14.6 (from flwr_datasets[vision])\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib<4.0.0,>=3.7.5 (from flwr_datasets[vision])\n",
            "  Downloading matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (1.25.2)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (0.13.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (4.66.4)\n",
            "Requirement already satisfied: pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from flwr_datasets[vision]) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision])\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision])\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision])\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision])\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (6.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (2.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.5->flwr_datasets[vision]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0,>=2.14.6->flwr_datasets[vision]) (2024.7.4)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, matplotlib, datasets, flwr_datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 flwr_datasets-0.2.0 matplotlib-3.9.1 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flwr[simulation] torch torchvision scipy\n",
        "!pip install flwr_datasets[vision]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pq6T1oFya-6"
      },
      "source": [
        "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGxlwHOya-6",
        "outputId": "e21ad23f-5a43-4614-8f0f-55228e46debd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu using PyTorch 2.3.1+cu121 and Flower 1.9.0\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import flwr as fl\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
        "print(\n",
        "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE9FDjOZya-6"
      },
      "source": [
        "It is possible to switch to a runtime that has GPU acceleration enabled (on Google Colab: `Runtime > Change runtime type > Hardware acclerator: GPU > Save`). Note, however, that Google Colab is not always able to offer GPU acceleration. If you see an error related to GPU availability in one of the following sections, consider switching back to CPU-based execution by setting `DEVICE = torch.device(\"cpu\")`. If the runtime has GPU acceleration enabled, you should see the output `Training on cuda`, otherwise it'll say `Training on cpu`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjuuhoxcya-7"
      },
      "source": [
        "### Data loading\n",
        "\n",
        "Let's now load the CIFAR-10 training and test set, partition them into ten smaller datasets (each split into training and validation set), and wrap everything in their own `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff8h-Bc9ya-7",
        "outputId": "a623d82e-d3ab-4bc2-f0fc-db280075d2fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "NUM_CLIENTS = 500\n",
        "\n",
        "\n",
        "def load_datasets(num_clients: int):\n",
        "    # Download and transform CIFAR-10 (train and test)\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        "    )\n",
        "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
        "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // num_clients\n",
        "    lengths = [partition_size] * num_clients\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = len(ds) // 10  # 10 % validation set\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
        "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
        "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
        "    testloader = DataLoader(testset, batch_size=32)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "\n",
        "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jXdBnto2JdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet12(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet12, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 3, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 3, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM2301L1ya-7"
      },
      "outputs": [],
      "source": [
        "def get_parameters(model: torch.nn.ModuleList) -> List[np.ndarray]:\n",
        "    \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
        "    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
        "\n",
        "\n",
        "def set_parameters(model: torch.nn.ModuleList, params: List[np.ndarray]):\n",
        "    \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "    params_dict = zip(model.state_dict().keys(), params)\n",
        "    state_dict = OrderedDict({k: torch.from_numpy(np.copy(v)) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(net, trainloader, epochs: int):\n",
        "    \"\"\"Train the network on the training set.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(net.parameters())\n",
        "    net.train()\n",
        "    for epoch in range(epochs):\n",
        "        correct, total, epoch_loss = 0, 0, 0.0\n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Metrics\n",
        "            epoch_loss += loss.item() * images.size(0)  # Summing the loss\n",
        "            total += labels.size(0)\n",
        "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
        "        epoch_loss /= total  # Averaging the loss over the entire dataset\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
        "\n",
        "def test(net, testloader):\n",
        "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct, total, loss = 0, 0, 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            loss += criterion(outputs, labels).item() * images.size(0)  # Summing the loss\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    loss /= total  # Averaging the loss over the entire dataset\n",
        "    accuracy = correct / total\n",
        "    return loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwiQEaqFya--"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from typing import cast\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "from flwr.common.typing import NDArray, NDArrays, Parameters\n",
        "\n",
        "\n",
        "def ndarrays_to_sparse_parameters(ndarrays: NDArrays) -> Parameters:\n",
        "    \"\"\"Convert NumPy ndarrays to parameters object.\"\"\"\n",
        "    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]\n",
        "    return Parameters(tensors=tensors, tensor_type=\"numpy.ndarray\")\n",
        "\n",
        "\n",
        "def sparse_parameters_to_ndarrays(parameters: Parameters) -> NDArrays:\n",
        "    \"\"\"Convert parameters object to NumPy ndarrays.\"\"\"\n",
        "    return [sparse_bytes_to_ndarray(tensor) for tensor in parameters.tensors]\n",
        "\n",
        "\n",
        "def ndarray_to_sparse_bytes(ndarray: NDArray) -> bytes:\n",
        "    \"\"\"Serialize NumPy ndarray to bytes.\"\"\"\n",
        "    bytes_io = BytesIO()\n",
        "\n",
        "    if len(ndarray.shape) > 1:\n",
        "        # Flatten higher-dimensional array to 2D\n",
        "        original_shape = ndarray.shape\n",
        "        ndarray = ndarray.reshape(-1, ndarray.shape[-1])\n",
        "\n",
        "        # Convert ndarray to sparse matrix\n",
        "        sparse_matrix = scipy.sparse.csr_matrix(ndarray)\n",
        "\n",
        "        np.savez(\n",
        "            bytes_io,\n",
        "            data=sparse_matrix.data,\n",
        "            indices=sparse_matrix.indices,\n",
        "            indptr=sparse_matrix.indptr,\n",
        "            shape=sparse_matrix.shape,\n",
        "            original_shape=original_shape,  # Store original shape for reshaping\n",
        "            allow_pickle=False,\n",
        "        )\n",
        "    else:\n",
        "        np.save(bytes_io, ndarray, allow_pickle=False)\n",
        "    return bytes_io.getvalue()\n",
        "\n",
        "\n",
        "def sparse_bytes_to_ndarray(tensor: bytes) -> NDArray:\n",
        "    \"\"\"Deserialize NumPy ndarray from bytes.\"\"\"\n",
        "    bytes_io = BytesIO(tensor)\n",
        "    loader = np.load(bytes_io, allow_pickle=False)\n",
        "\n",
        "    if \"indptr\" in loader:\n",
        "        # Convert sparse matrix back to ndarray\n",
        "        sparse_matrix = scipy.sparse.csr_matrix(\n",
        "            (loader[\"data\"], loader[\"indices\"], loader[\"indptr\"]),\n",
        "            shape=loader[\"shape\"]\n",
        "        )\n",
        "        ndarray_deserialized = sparse_matrix.toarray()\n",
        "\n",
        "        # Reshape back to original shape if needed\n",
        "        if \"original_shape\" in loader:\n",
        "            original_shape = loader[\"original_shape\"]\n",
        "            ndarray_deserialized = ndarray_deserialized.reshape(original_shape)\n",
        "    else:\n",
        "        ndarray_deserialized = loader\n",
        "    return cast(NDArray, ndarray_deserialized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pErPkjIDya--"
      },
      "source": [
        "### Client-side\n",
        "\n",
        "To be able to serialize our `ndarray`s into sparse parameters, we will just have to call our custom functions in our `flwr.client.Client`.\n",
        "\n",
        "Indeed, in `get_parameters` we need to serialize the parameters we got from our network using our custom `ndarrays_to_sparse_parameters` defined above.\n",
        "\n",
        "In `fit`, we first need to deserialize the parameters coming from the server using our custom `sparse_parameters_to_ndarrays` and then we need to\n",
        "serialize our local results with `ndarrays_to_sparse_parameters`.\n",
        "\n",
        "In `evaluate`, we will only need to deserialize the global parameters with our custom function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo97DTl9yhYE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "def prune_model_unstructured(model, pruning_method, **kwargs):\n",
        "    # Collect the parameters to prune\n",
        "    parameters_to_prune = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            parameters_to_prune.append((module, 'weight'))\n",
        "            if module.bias is not None:\n",
        "                parameters_to_prune.append((module, 'bias'))\n",
        "\n",
        "    # Apply global pruning\n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method=pruning_method,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    # Remove pruning reparameterization\n",
        "    for module, param in parameters_to_prune:\n",
        "        prune.remove(module, param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwCO1ygkya--"
      },
      "outputs": [],
      "source": [
        "from flwr.common import (\n",
        "    Code,\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    GetParametersIns,\n",
        "    GetParametersRes,\n",
        "    Status,\n",
        ")\n",
        "\n",
        "class FlowerClient(fl.client.Client):\n",
        "    def __init__(self, cid, net, trainloader, valloader, pruning_rate):\n",
        "        self.cid = cid\n",
        "        self.net = net\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = valloader\n",
        "        self.pruning_rate = pruning_rate\n",
        "\n",
        "    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:\n",
        "        print(f\"[Client {self.cid}] get_parameters\")\n",
        "\n",
        "        # Get parameters as a list of NumPy ndarray's\n",
        "        ndarrays: List[np.ndarray] = get_parameters(self.net)\n",
        "\n",
        "        # Serialize ndarray's into a Parameters object using our custom function\n",
        "        parameters = ndarrays_to_sparse_parameters(ndarrays)\n",
        "\n",
        "        # Build and return response\n",
        "        status = Status(code=Code.OK, message=\"Success\")\n",
        "        return GetParametersRes(\n",
        "            status=status,\n",
        "            parameters=parameters,\n",
        "        )\n",
        "\n",
        "    def fit(self, ins: FitIns) -> FitRes:\n",
        "        print(f\"[Client {self.cid}] fit, config: {ins.config}\")\n",
        "\n",
        "        # Deserialize parameters to NumPy ndarray's using our custom function\n",
        "        parameters_original = ins.parameters\n",
        "        ndarrays_original = sparse_parameters_to_ndarrays(parameters_original)\n",
        "\n",
        "        # Update local model, train, get updated parameters\n",
        "        set_parameters(self.net, ndarrays_original)\n",
        "        train(self.net, self.trainloader, epochs=1)\n",
        "        pruning_method = torch.nn.utils.prune.L1Unstructured\n",
        "        prune_model_unstructured(self.net, pruning_method, amount=self.pruning_rate)\n",
        "        ndarrays_updated = get_parameters(self.net)\n",
        "\n",
        "        # Serialize ndarray's into a Parameters object using our custom function\n",
        "        parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)\n",
        "        # Save the sparse matrix to an .npz file\n",
        "        bytes_sent = sum(len(tensor) for tensor in parameters_updated.tensors)\n",
        "\n",
        "        # Build and return response\n",
        "        status = Status(code=Code.OK, message=\"Success\")\n",
        "        return FitRes(\n",
        "            status=status,\n",
        "            parameters=parameters_updated,\n",
        "            num_examples=len(self.trainloader),\n",
        "            metrics={\"bytes sent\" : bytes_sent},\n",
        "        )\n",
        "\n",
        "    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:\n",
        "        print(f\"[Client {self.cid}] evaluate, config: {ins.config}\")\n",
        "\n",
        "        # Deserialize parameters to NumPy ndarray's using our custom function\n",
        "        parameters_original = ins.parameters\n",
        "        ndarrays_original = sparse_parameters_to_ndarrays(parameters_original)\n",
        "\n",
        "        set_parameters(self.net, ndarrays_original)\n",
        "        loss, accuracy = test(self.net, self.valloader)\n",
        "\n",
        "        # Build and return response\n",
        "        status = Status(code=Code.OK, message=\"Success\")\n",
        "        return EvaluateRes(\n",
        "            status=status,\n",
        "            loss=float(loss),\n",
        "            num_examples=len(self.valloader),\n",
        "            metrics={\"accuracy\": float(accuracy), \"loss\" : float(loss)},\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNEjOqwpyyjI"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "def get_evaluate_fn(testloader: Dataset):\n",
        "    \"\"\"This is a function that returns a function. The returned\n",
        "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
        "    at the end of each round to evaluate the stat of the global\n",
        "    model.\"\"\"\n",
        "\n",
        "    def evaluate_fn(server_round: int, parameters, config):\n",
        "        \"\"\"This function is executed by the strategy it will instantiate\n",
        "        a model and replace its parameters with those from the global model.\n",
        "        The, the model will be evaluate on the test set (recall this is the\n",
        "        whole MNIST test set).\"\"\"\n",
        "\n",
        "        model = ResNet12(num_classes=10)\n",
        "\n",
        "        # Determine device\n",
        "        model.to(DEVICE)  # send model to device\n",
        "\n",
        "        # set parameters to the model\n",
        "        params_dict = zip(model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.from_numpy(np.copy(v)) for k, v in params_dict})\n",
        "        model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "        loss, accuracy = test(model, testloader)\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHeJ1jZLya-_"
      },
      "outputs": [],
      "source": [
        "from logging import WARNING\n",
        "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from flwr.common import FitRes, MetricsAggregationFn, NDArrays, Parameters, Scalar\n",
        "from flwr.common.logger import log\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.strategy import FedAvg\n",
        "from flwr.server.strategy.aggregate import aggregate\n",
        "\n",
        "WARNING_MIN_AVAILABLE_CLIENTS_TOO_LOW = \"\"\"\n",
        "Setting `min_available_clients` lower than `min_fit_clients` or\n",
        "`min_evaluate_clients` can cause the server to fail when there are too few clients\n",
        "connected to the server. `min_available_clients` must be set to a value larger\n",
        "than or equal to the values of `min_fit_clients` and `min_evaluate_clients`.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FedSparse(FedAvg):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        fraction_fit: float = 1.0,\n",
        "        fraction_evaluate: float = 1.0,\n",
        "        min_fit_clients: int = 2,\n",
        "        min_evaluate_clients: int = 2,\n",
        "        min_available_clients: int = 2,\n",
        "        evaluate_fn: Optional[\n",
        "            Callable[\n",
        "                [int, NDArrays, Dict[str, Scalar]],\n",
        "                Optional[Tuple[float, Dict[str, Scalar]]],\n",
        "            ]\n",
        "        ] = None,\n",
        "        on_fit_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        on_evaluate_config_fn: Optional[Callable[[int], Dict[str, Scalar]]] = None,\n",
        "        accept_failures: bool = True,\n",
        "        initial_parameters: Optional[Parameters] = None,\n",
        "        fit_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "        evaluate_metrics_aggregation_fn: Optional[MetricsAggregationFn] = None,\n",
        "    ) -> None:\n",
        "        \"\"\"Custom FedAvg strategy with sparse matrices.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        fraction_fit : float, optional\n",
        "            Fraction of clients used during training. Defaults to 0.1.\n",
        "        fraction_evaluate : float, optional\n",
        "            Fraction of clients used during validation. Defaults to 0.1.\n",
        "        min_fit_clients : int, optional\n",
        "            Minimum number of clients used during training. Defaults to 2.\n",
        "        min_evaluate_clients : int, optional\n",
        "            Minimum number of clients used during validation. Defaults to 2.\n",
        "        min_available_clients : int, optional\n",
        "            Minimum number of total clients in the system. Defaults to 2.\n",
        "        evaluate_fn : Optional[Callable[[int, NDArrays, Dict[str, Scalar]], Optional[Tuple[float, Dict[str, Scalar]]]]]\n",
        "            Optional function used for validation. Defaults to None.\n",
        "        on_fit_config_fn : Callable[[int], Dict[str, Scalar]], optional\n",
        "            Function used to configure training. Defaults to None.\n",
        "        on_evaluate_config_fn : Callable[[int], Dict[str, Scalar]], optional\n",
        "            Function used to configure validation. Defaults to None.\n",
        "        accept_failures : bool, optional\n",
        "            Whether or not accept rounds containing failures. Defaults to True.\n",
        "        initial_parameters : Parameters, optional\n",
        "            Initial global model parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        if (\n",
        "            min_fit_clients > min_available_clients\n",
        "            or min_evaluate_clients > min_available_clients\n",
        "        ):\n",
        "            log(WARNING, WARNING_MIN_AVAILABLE_CLIENTS_TOO_LOW)\n",
        "\n",
        "        super().__init__(\n",
        "            fraction_fit=fraction_fit,\n",
        "            fraction_evaluate=fraction_evaluate,\n",
        "            min_fit_clients=min_fit_clients,\n",
        "            min_evaluate_clients=min_evaluate_clients,\n",
        "            min_available_clients=min_available_clients,\n",
        "            evaluate_fn=evaluate_fn,\n",
        "            on_fit_config_fn=on_fit_config_fn,\n",
        "            on_evaluate_config_fn=on_evaluate_config_fn,\n",
        "            accept_failures=accept_failures,\n",
        "            initial_parameters=initial_parameters,\n",
        "            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n",
        "            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n",
        "        )\n",
        "\n",
        "    def evaluate(\n",
        "        self, server_round: int, parameters: Parameters\n",
        "    ) -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
        "        \"\"\"Evaluate model parameters using an evaluation function.\"\"\"\n",
        "        if self.evaluate_fn is None:\n",
        "            # No evaluation function provided\n",
        "            return None\n",
        "\n",
        "        # We deserialize using our custom method\n",
        "        parameters_ndarrays = sparse_parameters_to_ndarrays(parameters)\n",
        "\n",
        "        eval_res = self.evaluate_fn(server_round, parameters_ndarrays, {})\n",
        "        if eval_res is None:\n",
        "            return None\n",
        "        loss, metrics = eval_res\n",
        "        return loss, metrics\n",
        "\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: List[Tuple[ClientProxy, FitRes]],\n",
        "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
        "        \"\"\"Aggregate fit results using weighted average.\"\"\"\n",
        "        if not results:\n",
        "            return None, {}\n",
        "        # Do not aggregate if there are failures and failures are not accepted\n",
        "        if not self.accept_failures and failures:\n",
        "            return None, {}\n",
        "\n",
        "        # We deserialize each of the results with our custom method\n",
        "        weights_results = [\n",
        "            (sparse_parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n",
        "            for _, fit_res in results\n",
        "        ]\n",
        "\n",
        "        # We serialize the aggregated result using our custom method\n",
        "        parameters_aggregated = ndarrays_to_sparse_parameters(\n",
        "            aggregate(weights_results)\n",
        "        )\n",
        "\n",
        "        # Aggregate custom metrics if aggregation fn was provided\n",
        "        metrics_aggregated = {}\n",
        "        if self.fit_metrics_aggregation_fn:\n",
        "            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n",
        "            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n",
        "        elif server_round == 1:  # Only log this warning once\n",
        "            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n",
        "\n",
        "        return parameters_aggregated, metrics_aggregated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ni7tJdezAB8"
      },
      "outputs": [],
      "source": [
        "from flwr.common import Metrics\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
        "    the client's evaluate() method.\"\"\"\n",
        "    # Multiply accuracy of each client by number of examples used\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "\n",
        "    # Aggregate and return custom metric (weighted average)\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "def aggregate_fit_metrics(metrics_list: List[Tuple[int, Dict[str, Scalar]]]) -> Dict[str, Scalar]:\n",
        "    \"\"\"Aggregate custom fit metrics from clients to calculate the average bytes sent.\n",
        "\n",
        "    Args:\n",
        "        metrics_list (List[Tuple[int, Dict[str, Scalar]]]): List of tuples, where each tuple\n",
        "        contains the number of examples and a dictionary of metrics from a client.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Scalar]: Aggregated metrics containing the average bytes sent.\n",
        "    \"\"\"\n",
        "    total_bytes_sent = 0\n",
        "    num_clients = len(metrics_list)\n",
        "\n",
        "    for _, metrics in metrics_list:\n",
        "        total_bytes_sent += metrics[\"bytes sent\"]\n",
        "\n",
        "    # Calculate the average bytes sent\n",
        "    average_bytes_sent = total_bytes_sent / num_clients if num_clients > 0 else 0\n",
        "\n",
        "    # Create the aggregated metrics dictionary\n",
        "    aggregated_metrics = {\n",
        "        \"bytes sent\": average_bytes_sent,\n",
        "    }\n",
        "\n",
        "    return aggregated_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I9xIadcya-_"
      },
      "source": [
        "We can now run our custom serialization example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTyPa5Bbya-_",
        "outputId": "a95d1316-81d0-453d-c3d8-d672a16dfa1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=15, no round_timeout\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running simulation with pruning rate: 0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "2024-07-22 10:45:51,602\tINFO worker.py:1752 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'object_store_memory': 4003658956.0, 'memory': 8007317915.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'CPU': 2.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      No `client_resources` specified. Using minimal resources for clients.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 0.0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=7379)\u001b[0m 2024-07-22 10:45:58.581273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=7379)\u001b[0m 2024-07-22 10:45:58.581386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=7379)\u001b[0m 2024-07-22 10:45:58.585641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=7379)\u001b[0m 2024-07-22 10:46:00.921040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] get_parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Evaluating initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 2.3022777770996092, {'accuracy': 0.1}\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.5448879771762423, accuracy 0.1111111111111111\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.57949481010437, accuracy 0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (1, 2.3057404636383056, {'accuracy': 0.1}, 447.3851304220002)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.359609874089559, accuracy 0.13333333333333333\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (2, 2.3100083656311035, {'accuracy': 0.1}, 891.0967990170002)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.442140950096978, accuracy 0.1111111111111111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.335446018642849, accuracy 0.13333333333333333\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (3, 2.3131433921813964, {'accuracy': 0.1}, 1332.852181278)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.2436572127872045, accuracy 0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.1827552954355878, accuracy 0.23333333333333334\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (4, 2.3149128650665283, {'accuracy': 0.1}, 1767.574592986)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.233248080147637, accuracy 0.18888888888888888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.111524004406399, accuracy 0.24444444444444444\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (5, 2.3157820350646974, {'accuracy': 0.1}, 2201.8391276700004)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.066506740781996, accuracy 0.28888888888888886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 6]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 2.104243368572659, accuracy 0.23333333333333334\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (6, 2.3170186695098876, {'accuracy': 0.1001}, 2628.744768513)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 0] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.000905079311795, accuracy 0.3111111111111111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 7]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.9698669698503282, accuracy 0.36666666666666664\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (7, 2.31874292678833, {'accuracy': 0.1001}, 3050.530117028)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] evaluate, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m Epoch 1: train loss 2.007309971915351, accuracy 0.26666666666666666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 8]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.896891540951199, accuracy 0.4111111111111111\n",
            "\u001b[36m(ClientAppActor pid=7378)\u001b[0m [Client 1] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 1 failures\n",
            "\u001b[33m(raylet)\u001b[0m [2024-07-22 11:44:51,663 E 7234 7234] (raylet) node_manager.cc:2967: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[36m(pid=7378)\u001b[0m 2024-07-22 10:45:58.581277: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=7378)\u001b[0m 2024-07-22 10:45:58.581386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=7378)\u001b[0m 2024-07-22 10:45:58.585641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=7378)\u001b[0m 2024-07-22 10:46:00.921040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (8, 2.3191451866149904, {'accuracy': 0.1001}, 3470.788359653)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 9]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.7758940246370103, accuracy 0.6222222222222222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (9, 2.319332190322876, {'accuracy': 0.0985}, 3892.5188222799998)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 10]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.690332579612732, accuracy 0.7333333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (10, 2.319264129638672, {'accuracy': 0.12}, 4310.172409032)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 11]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 0] fit, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.6174728870391846, accuracy 0.8333333333333334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      fit progress: (11, 2.315991773223877, {'accuracy': 0.129}, 4723.603279751)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] evaluate, config: {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 1 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 12]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 73, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 399, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 280, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2667, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: ac254fec048196f99fc5bbe287621a7fa05286f148ca1ea2aa7943cc) where the task (actor ID: 7ac8404e52405057276f92eb01000000, name=ClientAppActor.__init__, pid=7378, memory used=3.21GB) was running was 12.05GB / 12.67GB (0.951039), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-dd3b69f5ac661e31c6ba1671320900cb4030dd5494074fbdbf3da3d9*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "448\t3.49\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-dfebba56-2fe2...\n",
            "7378\t3.21\tray::ClientAppActor.run\n",
            "7379\t2.98\tray::ClientAppActor.run\n",
            "84\t0.08\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "7\t0.06\t/tools/node/bin/node /datalab/web/app.js\n",
            "7202\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "7281\t0.05\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1322\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "7201\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "7256\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m [Client 1] fit, config: {}\n",
            "\u001b[36m(ClientAppActor pid=7379)\u001b[0m Epoch 1: train loss 1.981304449505276, accuracy 0.3333333333333333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 1 failures\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "strategy = FedSparse(evaluate_metrics_aggregation_fn = weighted_average,\n",
        "                     fit_metrics_aggregation_fn = aggregate_fit_metrics,\n",
        "                     evaluate_fn=get_evaluate_fn(testloader))\n",
        "\n",
        "client_resources = None\n",
        "log_data = []\n",
        "if DEVICE.type == \"cuda\":\n",
        "    client_resources = {\"num_gpus\": 1}\n",
        "\n",
        "for pruning_rate in np.arange(0.1, 1.0, 0.4):\n",
        "    print(f\"Running simulation with pruning rate: {pruning_rate}\")\n",
        "\n",
        "    def client_fn(cid) -> FlowerClient:\n",
        "          net = ResNet12(num_classes=10).to(DEVICE)\n",
        "          trainloader = trainloaders[int(cid)]\n",
        "          valloader = valloaders[int(cid)]\n",
        "          return FlowerClient(cid, net, trainloader, valloader, pruning_rate)\n",
        "\n",
        "    history = fl.simulation.start_simulation(\n",
        "        strategy=strategy,\n",
        "        client_fn=client_fn,\n",
        "        num_clients=2,\n",
        "        config=fl.server.ServerConfig(num_rounds=15),\n",
        "        client_resources=client_resources,\n",
        "    )\n",
        "\n",
        "    # Collect the metrics\n",
        "    log_entry = {\n",
        "        \"pruning_rate\": pruning_rate,\n",
        "        \"history_loss_distributed\": history.losses_distributed,\n",
        "        \"history_loss_centralized\": history.losses_centralized,\n",
        "        \"history_metrics_distributed_fit\": history.metrics_distributed_fit,\n",
        "        \"history_metrics_distributed_evaluate\": history.metrics_distributed,\n",
        "        \"history_metrics_centralized\": history.metrics_centralized\n",
        "    }\n",
        "    log_data.append(log_entry)\n",
        "\n",
        "# Write the collected log data to a JSON file\n",
        "with open('CIFAR10_ResNet12_Unstructured.json', 'w') as f:\n",
        "    json.dump(log_data, f, indent=4)\n",
        "\n",
        "print(\"Metrics logged\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}